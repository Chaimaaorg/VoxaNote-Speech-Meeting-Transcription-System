# ğŸ—£ï¸ End-to-End Speech-to-Text Meeting Transcription System

This project is a complete **AI-powered meeting transcription assistant** that converts audio recordings from meetings into accurate written notes using a custom-trained DeepSpeech2 model with **CTC loss** in TensorFlow.

It includes:

* ğŸ™ï¸ Audio preprocessing & segmentation
* ğŸ¤– DeepSpeech2-based ASR (Automatic Speech Recognition)
* ğŸ“‹ LLM-based output formatting
* ğŸŒ React frontend + FastAPI backend

![System Overview](./artifacts/demo.png)

**Kaggle Notebook (Model training)**:
ğŸ”— [End-to-End ASR with DeepSpeech2 & CTC Loss (TensorFlow)](https://www.kaggle.com/code/chaimaourgani/end-to-end-asr-with-deepspeech2-ctc-loss-tf)

---

## ğŸš€ Features

* âœ… Upload long meeting audio files
* âœ… Audio segmentation for efficient inference
* âœ… DeepSpeech2 ASR pipeline with CTC decoding
* âœ… Output post-processed with optional LLM formatting
* âœ… Clean React-based UI for testing the system

---

## ğŸ§  Project Structure

```
project-root/
â”‚
â”œâ”€â”€ frontend/                  # React web app for uploading & viewing results
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ App.js
â”‚       â””â”€â”€ index.js
â”‚
â”œâ”€â”€ backend/                   
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ constants.py           # Constants, configs
â”‚   â”‚   â”œâ”€â”€ llm_formater.py        # Post-processing with LLM
â”‚   â”‚   â”œâ”€â”€ process_audio.py       # Audio preprocessing
â”‚   â”‚   â””â”€â”€ setup_model.py         # Model loading
â”‚   â””â”€â”€ main.py                    # FastAPI entry point
â”‚
â”œâ”€â”€ artifacts/                  # Model weights, logs, etc.
â”œâ”€â”€ requirements.txt            # Backend Python dependencies
â”œâ”€â”€ README.md                   # You're here
```

---

## âš™ï¸ Setup Instructions

### ğŸ”§ Backend

```bash
cd backend/
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows
pip install -r requirements.txt
uvicorn main:app --reload
```

> Make sure to place your trained `.keras` model and `vocab.json` in the appropriate paths (`artifacts/` or `src/`).

### ğŸ–¼ï¸ Frontend

```bash
cd frontend/
npm install
npm start
```

---

## ğŸ“¦ API Usage

### POST `/transcribe/`

Upload a WAV audio file to get the transcription.

#### Example with `curl`:

```bash
curl -X POST "http://localhost:8000/transcribe/" \
  -H "accept: application/json" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@meeting.wav"
```

---

## ğŸ“ˆ Model Overview

The ASR model is based on **DeepSpeech2**, using:

* **CTC loss**
* **Bidirectional RNNs**
* **Custom character vocabulary**
* Trained on **LibriSpeech-like data**
  *(See the [Kaggle notebook](https://www.kaggle.com/code/chaimaourgani/end-to-end-asr-with-deepspeech2-ctc-loss-tf) for details)*

---

## ğŸ” Future Work

* ğŸ” Real-time streaming support
* ğŸ“„ LLM summarization of transcripts
* ğŸ“Š Analytics dashboard for meetings
* ğŸŒ Multilingual support

---

## ğŸ§‘â€ğŸ’» Author

**ChaimaÃ¢ Ourgani**
ğŸ“« [chaimaaorg.github.io/portfolio](https://chaimaaorg.github.io/portfolio/)
ğŸ”— [LinkedIn](https://www.linkedin.com/in/chaima%C3%A2-ourgani-65a422273/)

---

## ğŸ“œ License

MIT License â€“ use freely and responsibly.
